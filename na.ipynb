{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "import time\n",
    "#MongoDB access\n",
    "uri1 = \"mongodb+srv://xinmeng:xxm12345@jobcandidate.g9tuz.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\"\n",
    "client1_ = MongoClient(uri1)\n",
    "# database name CRM\n",
    "DB1_ = client1_['CRM']\n",
    "\n",
    "# Initialize the WebDriver\n",
    "def scratch_data(search_char,collection):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    # Open the URL\n",
    "    url = 'https://apps.nd.gov/sc/busnsrch/busnSearch.htm'\n",
    "    driver.get(url)\n",
    "\n",
    "    # Select the dropdown option for \"Entity Name starts with\"\n",
    "    dropdown_element = driver.find_element(By.ID, 'srchType')\n",
    "    dropdown = Select(dropdown_element)\n",
    "    dropdown.select_by_visible_text(\"Entity Name starts with\")\n",
    "    time.sleep(2)\n",
    "    entity_name_input = driver.find_element(By.ID, 'searchName')\n",
    "    entity_name_input.send_keys(search_char)\n",
    "\n",
    "    # Click the \"Search\" button\n",
    "    search_button = driver.find_element(By.XPATH, '//input[@value=\"Search\"]')\n",
    "    search_button.click()\n",
    "    page=1\n",
    "\n",
    "    # Loop through all pages by clicking the \"Next\" button\n",
    "    while True:\n",
    "        try:\n",
    "            wait = WebDriverWait(driver, 20)\n",
    "            table = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.tabledisplaydata table[summary='Businesses returned in search results']\")))\n",
    "\n",
    "            # Find all the rows in the table that contain company data\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")   \n",
    "            page_total_rows=len(rows[1:])\n",
    "            #print(page_total_rows)\n",
    "            # Loop through each row and extract company information\n",
    "            for i in range(page_total_rows):\n",
    "                try:\n",
    "                    columns = rows[i+1].find_elements(By.TAG_NAME, \"td\")\n",
    "                    valid_data=any(column.text.strip() and column.text.strip() != \"\\xa0\" for column in columns)\n",
    "                    if valid_data:\n",
    "                        link = columns[2].find_element(By.TAG_NAME, \"a\")\n",
    "                        link.click()\n",
    "                        wait.until(EC.presence_of_element_located((By.ID, 'BusnSrchFM')))  # Wait for the form\n",
    "                        html = driver.page_source\n",
    "                        soup = BeautifulSoup(html, 'html.parser')\n",
    "                        #Extract the dynamic key-value pairs\n",
    "                        data_dict = {}\n",
    "                        # Extract company name (always exists in the h3 tag)\n",
    "                        company_name = soup.find('h3').get_text(strip=True)\n",
    "                        data_dict[\"company_name\"] = company_name\n",
    "                        # Extract the corporation details in the table\n",
    "                        details = soup.find_all('li')\n",
    "                        # Loop through the <li> elements to extract dynamic keys and values\n",
    "                        for detail in details:\n",
    "                            text = detail.get_text(strip=True)\n",
    "                            if ':' in text:\n",
    "                                key, value = text.split(\":\", 1)\n",
    "                                data_dict[key.strip()] = value.strip()\n",
    "                        nature_of_business = soup.find('h4', text=\"Nature of Business\")\n",
    "                        if nature_of_business:\n",
    "                            nature_of_business_div = nature_of_business.find_next('div', class_='description')\n",
    "                            if nature_of_business_div:\n",
    "                                data_dict[\"nature_of_business\"] = nature_of_business_div.get_text(strip=True)\n",
    "                        managing_partners_heading = soup.find('h4', text=\"General/Managing Partners\")\n",
    "                        if managing_partners_heading:\n",
    "                            managing_partners_div = managing_partners_heading.find_next('div', class_='address')\n",
    "                            if managing_partners_div:\n",
    "                                managing_partners = managing_partners_div.get_text(\" \", strip=True)\n",
    "                                data_dict[\"general_managing_partners\"] = managing_partners\n",
    "                       \n",
    "                        principal_office_heading = soup.find('h4', text=\"Principal Office\")\n",
    "                        if principal_office_heading:\n",
    "                            principal_office_div = principal_office_heading.find_next('div', class_='address')\n",
    "                            if principal_office_div:\n",
    "                                principal_office = principal_office_div.get_text(\" \", strip=True)\n",
    "                                data_dict[\"principal_office\"] = principal_office\n",
    "                        \n",
    "                        registered_agent_heading = soup.find('h4', text=\"Registered Agent\")\n",
    "                        if registered_agent_heading:\n",
    "                            registered_agent_div = registered_agent_heading.find_next('div', class_='address')\n",
    "                            if registered_agent_div:\n",
    "                                registered_agent = registered_agent_div.get_text(\" \", strip=True)\n",
    "                                data_dict[\"registered_agent\"] = registered_agent\n",
    "                        # Extract Authorized Shares (if exists)\n",
    "                        authorized_shares_heading = soup.find('h4', text=\"Authorized Shares\")\n",
    "                        if authorized_shares_heading:\n",
    "                            authorized_shares_section = authorized_shares_heading.find_next('div', class_='tabledisplaydataindent')\n",
    "                            if authorized_shares_section:\n",
    "                                shares_rows = authorized_shares_section.find_all('tr')\n",
    "                                if len(shares_rows) > 1:  # Ensure there is at least a second row with data\n",
    "                                    authorized_shares = shares_rows[1].find_all('td')\n",
    "                                    if len(authorized_shares) >= 3:  # Ensure enough columns exist\n",
    "                                        data_dict[\"authorized_shares\"] = {\n",
    "                                            \"number\": authorized_shares[1].get_text(strip=True),\n",
    "                                            \"par_value\": authorized_shares[2].get_text(strip=True)\n",
    "                                        }\n",
    "                        \n",
    "                        business_address_heading = soup.find('h4', text=\"Business Address\")\n",
    "                        if business_address_heading:\n",
    "                            business_address_div = business_address_heading.find_next('div', class_='address')\n",
    "                            if business_address_div:\n",
    "                                business_address = business_address_div.get_text(\" \", strip=True)\n",
    "                                data_dict[\"business_address\"] = business_address\n",
    "                        # Extract License Details (if they exist)\n",
    "                        license_details_heading = soup.find('h4', text=\"License Details\")\n",
    "                        if license_details_heading:\n",
    "                            license_details_div = license_details_heading.find_next('div', class_='address')\n",
    "                            if license_details_div:\n",
    "                                license_details = license_details_div.get_text(\" \", strip=True)\n",
    "                                data_dict[\"license_details\"] = license_details\n",
    "                                \n",
    "                        owners_heading = soup.find('h4', text=\"Owners\")\n",
    "                        if owners_heading:\n",
    "                            owners_div = owners_heading.find_next('div', class_='address')\n",
    "                            if owners_div:\n",
    "                                owners_info = owners_div.get_text(\" \", strip=True)  # Extract owner information as a single string\n",
    "                                data_dict[\"owners\"] = owners_info\n",
    "\n",
    "                        collection.insert_one(data_dict)\n",
    "                        driver.back()\n",
    "\n",
    "                        # After navigating back, wait for the table to reappear\n",
    "                        table = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.tabledisplaydata table[summary='Businesses returned in search results']\")))\n",
    "\n",
    "                        rows = table.find_elements(By.TAG_NAME, \"tr\") \n",
    "                        #print(i)\n",
    "                        #print(data_dict)\n",
    "\n",
    "            \n",
    "\n",
    "                except IndexError as e:\n",
    "                    # Handle the IndexError and print a message, continue with the next iteration\n",
    "                    print(f\"IndexError occurred at row {i + 1}: {str(e)}. Skipping to the next row.\")\n",
    "                    continue\n",
    "                \n",
    "                except Exception as e:\n",
    "                \n",
    "                    print(f\"An error occurred: {str(e)}. Skipping to the next row.\")\n",
    "                    continue\n",
    "            page+=1\n",
    "            next_page_link = driver.find_element(By.XPATH, f\"//a[contains(@href, 'navP{page}')]\")\n",
    "            next_page_link.click()\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(\"No more pages to load.\")\n",
    "            break\n",
    "\n",
    "    # Close the browser after scraping is done\n",
    "    driver.quit()\n",
    "\n",
    "    print(\"Data extraction completed and saved to company_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "def run_in_threads(search_chars, collection, max_threads=5):\n",
    "    threads = []\n",
    "    \n",
    "    for char in search_chars:\n",
    "        # Create and start a thread for each character\n",
    "        thread = threading.Thread(target=scratch_data, args=(char, collection))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "        # Check if we have reached the maximum number of threads\n",
    "        if len(threads) >= max_threads:\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "            threads = []  # Clear the list of threads after they have completed\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection=DB1_.ND\n",
    "search_chars = [chr(i) for i in range(65, 91)]\n",
    "run_in_threads(search_chars,collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kangqi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
